{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/\"\n",
    "pretrained_type = 'bert-base-uncased'\n",
    "seed = 42\n",
    "\n",
    "test_size = 0.1\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH + \"Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_sequence = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df[\"Text\"].values.tolist()\n",
    "labels = df[\"Score\"].values.astype(int)\n",
    "labels = (labels > 3).astype(int) # Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[0:1000]\n",
    "labels = labels[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_val, labels_train, labels_val = train_test_split(text, labels, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reviews(Dataset):\n",
    "    def __init__(self, text, labels):\n",
    "        self.text = text\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "        self.len = len(text)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_type)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt = re.sub('<[^<]+?>', '', self.text[idx]) # Removing html-tags\n",
    "        tokens = self.tokenizer.encode(txt, add_special_tokens=True)\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        label = self.labels[idx]\n",
    "        return tokens, label\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    labels = torch.tensor([b[1] for b in batch])\n",
    "\n",
    "    lengths = [len(b[0]) for b in batch]\n",
    "    max_length = min([max(lengths), max_length_sequence])\n",
    "\n",
    "    attention_mask = torch.zeros((len(batch), max_length), dtype=torch.int)\n",
    "    idx_tensor = torch.zeros((len(batch), max_length), dtype=torch.long)\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        batch_len = lengths[i]\n",
    "        batch_len = min([max_length_sequence, batch_len])\n",
    "\n",
    "        attention_mask[i, 0:batch_len] = 1\n",
    "        idx_tensor[i, 0:batch_len] = batch[i][0][0:batch_len]\n",
    "\n",
    "    return idx_tensor, attention_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = Reviews(text_train, labels_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = Reviews(text_val, labels_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nanmean(v, *args, inplace=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Taking mean of tensor with nan's, excluding them from the computation.\n",
    "    \"\"\"\n",
    "\n",
    "    if not inplace:\n",
    "        v = v.clone()\n",
    "    is_nan = torch.isnan(v)\n",
    "    v[is_nan] = 0\n",
    "    return v.sum(*args, **kwargs) / (~is_nan).float().sum(*args, **kwargs)\n",
    "\n",
    "def nanmax(v, *args, inplace=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Taking max of tensor with nan's, excluding them from the computation.\n",
    "    \"\"\"\n",
    "\n",
    "    if not inplace:\n",
    "        v = v.clone()\n",
    "    is_nan = torch.isnan(v)\n",
    "    v[is_nan] = -float(\"inf\")\n",
    "    return v.max(*args, **kwargs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, encoder_dim=768):\n",
    "        super().__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(pretrained_type)\n",
    "        self.change_freezing()\n",
    "        self.linear = nn.Linear(2 * encoder_dim, 1)\n",
    " \n",
    "    def change_freezing(self, mode=False):\n",
    "        for param in self.bert_model.parameters():\n",
    "            param.requires_grad = mode\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "\n",
    "        for m in self.bert_model.modules():\n",
    "            if isinstance(m, nn.Dropout):\n",
    "                m.eval()\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                m.eval()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        seq = self.bert_model(x)[0]\n",
    "        seq[~mask.bool()] = float(\"nan\")\n",
    "\n",
    "        output = torch.cat([\n",
    "            nanmean(seq, axis=1),\n",
    "            nanmax(seq, axis=1)\n",
    "            ], dim=1)\n",
    "\n",
    "        output = self.linear(output)        \n",
    "        return output[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentClassifier()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "lr = 1e-3\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss, val_loss, val_acc = 0.0, 0.0, 0.0\n",
    "\n",
    "    model.train()\n",
    "    for _, (idx_tensor, attention_mask, labels) in enumerate(train_dataloader):\n",
    "        idx_tensor, attention_mask, labels = idx_tensor.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(idx_tensor, attention_mask)\n",
    "\n",
    "        batch_loss = loss_fct(output, labels)        \n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += batch_loss.detach().cpu().numpy()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, (idx_tensor, attention_mask, labels) in enumerate(val_dataloader):\n",
    "            idx_tensor, attention_mask, labels = idx_tensor.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            output = model(idx_tensor, attention_mask)\n",
    "            batch_loss = loss_fct(output, labels)\n",
    "            val_loss += batch_loss.detach().cpu().numpy()\n",
    "\n",
    "            y_hat = (torch.sigmoid(output) > 0.5).long()\n",
    "            batch_acc = (y_hat == labels).float().mean()\n",
    "            val_acc += batch_acc.cpu().detach().numpy()\n",
    "\n",
    "    train_loss = np.round(train_loss / len(train_dataloader), 6)\n",
    "    val_loss = np.round(val_loss / len(val_dataloader), 6)\n",
    "    val_acc = np.round(val_acc / len(val_dataloader), 6)\n",
    "\n",
    "    print(f\"----------- Epoch {epoch} -----------\")\n",
    "    print(f\"Train loss: {train_loss}\")\n",
    "    print(f\"Validation loss: {val_loss}\")\n",
    "    print(f\"Validation accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
