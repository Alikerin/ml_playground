{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/\"\n",
    "seed = 42\n",
    "device = \"cuda\"\n",
    "\n",
    "embedding_dim = 300\n",
    "test_size = 0.1\n",
    "max_length_sequence = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH + \"Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df[\"Text\"].values.tolist()\n",
    "labels = df[\"Score\"].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we would like to do positive / negative sentiment prediction we will remove review value 3 since\n",
    "# it can be viewed as neutral\n",
    "\n",
    "text = [text[i] for i in range(len(text)) if labels[i] != 3]\n",
    "labels = np.array([labels[i] for i in range(len(labels)) if labels[i] != 3])\n",
    "labels = (labels > 3).astype(int) # Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub-sampling since it would take too long to train otherwise on my computer\n",
    "num_samples = 10000\n",
    "\n",
    "np.random.seed(seed)\n",
    "idx = np.random.choice(np.arange(len(text)), size=num_samples, replace=False)\n",
    "text = [text[i] for i in idx]\n",
    "labels = labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_val, labels_train, labels_val = train_test_split(text, labels, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reviews(Dataset):\n",
    "    def __init__(self, text, labels):\n",
    "        self.len = len(text)\n",
    "        self.tokenizer = spacy.load(\"en_core_web_md\")\n",
    "        self.vectors = self.text_to_vectors(text)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "    def text_to_vectors(self, text):\n",
    "        vector_list = []\n",
    "        \n",
    "        for txt in text:\n",
    "            txt = BeautifulSoup(txt).get_text().lower() \n",
    "            tokens = self.tokenizer(txt)\n",
    "\n",
    "            vectors = []\n",
    "            for token in tokens:\n",
    "                if not token.is_oov:\n",
    "                    vectors.append(token.vector)\n",
    "\n",
    "            vectors = np.vstack(vectors)\n",
    "            vectors = torch.from_numpy(vectors).float()\n",
    "            vector_list.append(vectors)\n",
    "        return vector_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vector = self.vectors[idx]\n",
    "        label = self.labels[idx]\n",
    "        return vector, label\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    labels = torch.tensor([b[1] for b in batch])\n",
    "\n",
    "    lengths = [len(b[0]) for b in batch]\n",
    "    max_length = min([max(lengths), max_length_sequence])\n",
    "\n",
    "    vector_tensor = torch.zeros((len(batch), max_length, embedding_dim))\n",
    "    mask = torch.zeros((len(batch), max_length, 1), dtype=torch.int)\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        batch_len = lengths[i]\n",
    "        batch_len = min([max_length_sequence, batch_len])\n",
    "        \n",
    "        mask[i, (max_length - batch_len):, :] = 1\n",
    "        vector_tensor[i, (max_length - batch_len):, :] = batch[i][0][0:batch_len, :]\n",
    "\n",
    "    return vector_tensor, mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = Reviews(text_train, labels_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = Reviews(text_val, labels_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, p=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "        self.linear = nn.Linear(3 * hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        output, _ = self.lstm(x)\n",
    "        mask = mask.repeat(1, 1, output.shape[-1])\n",
    "        \n",
    "        # Concat the last hidden output, mean & max over all hidden outputs.\n",
    "        output = torch.cat([\n",
    "            torch.sum(mask * output, dim=1) / torch.sum(mask, dim=1),\n",
    "            torch.max(mask * output, dim=1)[0], # Assuming 0 won't be max\n",
    "            output[:, -1, :]\n",
    "        ], dim=1)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        return output[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = Model()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "lr = 1e-3\n",
    "wd = 1e-4\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Epoch 0 -----------\n",
      "Train loss: 0.428948\n",
      "Validation loss: 0.387902\n",
      "Validation accuracy: 0.837515\n",
      "----------- Epoch 1 -----------\n",
      "Train loss: 0.32849\n",
      "Validation loss: 0.312759\n",
      "Validation accuracy: 0.857948\n",
      "----------- Epoch 2 -----------\n",
      "Train loss: 0.271902\n",
      "Validation loss: 0.269043\n",
      "Validation accuracy: 0.887019\n",
      "----------- Epoch 3 -----------\n",
      "Train loss: 0.235976\n",
      "Validation loss: 0.260857\n",
      "Validation accuracy: 0.878681\n",
      "----------- Epoch 4 -----------\n",
      "Train loss: 0.222214\n",
      "Validation loss: 0.236063\n",
      "Validation accuracy: 0.896484\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "for epoch in range(epochs):\n",
    "    train_loss, val_loss, val_acc = 0.0, 0.0, 0.0\n",
    "\n",
    "    model.train()\n",
    "    for _, (vectors_tensor, mask, labels) in enumerate(train_dataloader):\n",
    "        vectors_tensor, mask, labels = vectors_tensor.to(device), mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(vectors_tensor, mask)\n",
    "\n",
    "        batch_loss = loss_fct(output, labels)        \n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += batch_loss.detach().cpu().numpy()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, (vectors_tensor, mask, labels) in enumerate(val_dataloader):\n",
    "            vectors_tensor, mask, labels = vectors_tensor.to(device), mask.to(device), labels.to(device)\n",
    "\n",
    "            output = model(vectors_tensor, mask)\n",
    "            batch_loss = loss_fct(output, labels)\n",
    "            val_loss += batch_loss.cpu().numpy()\n",
    "\n",
    "            y_hat = (torch.sigmoid(output) > 0.5).long()\n",
    "            batch_acc = (y_hat == labels).float().mean()\n",
    "            val_acc += batch_acc.cpu().numpy()\n",
    "\n",
    "    train_loss = np.round(train_loss / len(train_dataloader), 6)\n",
    "    val_loss = np.round(val_loss / len(val_dataloader), 6)\n",
    "    val_acc = np.round(val_acc / len(val_dataloader), 6)\n",
    "\n",
    "    print(f\"----------- Epoch {epoch} -----------\")\n",
    "    print(f\"Train loss: {train_loss}\")\n",
    "    print(f\"Validation loss: {val_loss}\")\n",
    "    print(f\"Validation accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
