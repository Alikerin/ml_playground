{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import display\n",
    "from src import data_transformer as dt\n",
    "from src import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"data/VOCdevkit/VOC2012/JPEGImages/\"\n",
    "seed = 42\n",
    "target_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "C = 20\n",
    "S = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_path = \"data/pascal_train2012.json\"\n",
    "val_json_path = \"data/pascal_val2012.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cat, train_list = dt.load_pascal(train_json_path)\n",
    "_, val_list = dt.load_pascal(val_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = dt.rescale_bounding_boxes(train_list, target_size)\n",
    "val_list = dt.rescale_bounding_boxes(val_list, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = dt.convert_to_center(train_list)\n",
    "val_list = dt.convert_to_center(val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = train_list[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_str = img_path + example[0]\n",
    "img = display.read_img(img_str, target_size)\n",
    "\n",
    "img = display.draw_grid(img, int(np.ceil(target_size / S)))\n",
    "img = display.draw_boxes(img, example[3])\n",
    "img = display.draw_text(img, example[1], example[3])\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yolo_target(data_list, S, B, C, target_size):    \n",
    "    grid_size = target_size / S\n",
    "\n",
    "    y_list = []\n",
    "\n",
    "    for data in data_list: \n",
    "        # For one grid element: [pc, c1, ..., cN, b1x, b1y, w1, h1, b2x, b2y, w2, h2]\n",
    "        y = np.zeros((1, S, S, 5*B + C))\n",
    "        \n",
    "        classes = data[2]\n",
    "        boxes = data[3]\n",
    "\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "\n",
    "            center_width = box[0]\n",
    "            center_height = box[1]\n",
    "\n",
    "            width_index = int(np.floor((center_width / target_size) * S))\n",
    "            height_index = int(np.floor((center_height / target_size) * S))\n",
    "\n",
    "            class_index = classes[i]\n",
    "\n",
    "            # Normalize width and height by the enitre image size\n",
    "            w = box[2] / target_size\n",
    "            h = box[3] / target_size\n",
    "\n",
    "            # Normalize bx and by according to the current grid position\n",
    "            cx = (center_width - ((width_index / S) * target_size)) / grid_size\n",
    "            cy = (center_height - ((height_index / S) * target_size)) / grid_size\n",
    "\n",
    "            y[:, width_index, height_index, class_index] = 1.0\n",
    "\n",
    "            # Hardcoded for B=2\n",
    "            if y[:, width_index, height_index, 20] == 0.0:\n",
    "                y[:, width_index, height_index, 20] = 1.0\n",
    "                y[:, width_index, height_index, 21] = cx\n",
    "                y[:, width_index, height_index, 22] = cy\n",
    "                y[:, width_index, height_index, 23] = w\n",
    "                y[:, width_index, height_index, 24] = h\n",
    "            else:\n",
    "                y[:, width_index, height_index, 25] = 1.0\n",
    "                y[:, width_index, height_index, 26] = cx\n",
    "                y[:, width_index, height_index, 27] = cy\n",
    "                y[:, width_index, height_index, 28] = w\n",
    "                y[:, width_index, height_index, 29] = h\n",
    "\n",
    "        y_list.append(y)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = create_yolo_target(train_list, S, B, C, target_size)\n",
    "y_val = create_yolo_target(val_list, S, B, C, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PascalData(Dataset):\n",
    "    def __init__(self, data_list, y, lambda_indicator, target_size=target_size, path=img_path):\n",
    "        self.target_size = target_size\n",
    "        self.path = path\n",
    "        \n",
    "        self.lambda_indicator = torch.from_numpy(lambda_indicator).float()\n",
    "        \n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        self.file_list = [i[0] for i in data_list]      \n",
    "        \n",
    "        self.mean = np.array([0.485, 0.456, 0.406]).reshape((1,1,3))\n",
    "        self.std = np.array([0.229, 0.224, 0.225]).reshape((1,1,3))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_str = self.path + self.file_list[idx]\n",
    "        \n",
    "        img = display.read_img(img_str, self.target_size)\n",
    "        img = img / 255.0\n",
    "        img = (img - self.mean) / self.std\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        img = torch.from_numpy(img).float().to(device)\n",
    "        \n",
    "        temp_y = self.y[idx,:].to(device)\n",
    "        temp_lambda = self.lambda_indicator[idx, :, :, :].to(device)\n",
    "        \n",
    "        return (img, temp_y, temp_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lambda_indicator(y, lambda_coord=5.0, lambda_noobj=0.5):\n",
    "    lambda_indicator = (y > 0.0).astype(float)\n",
    "\n",
    "    lambda_indicator[:, :, :, 0:20][(lambda_indicator[:, :, :, 20] != 0.0) | \n",
    "                                    (lambda_indicator[:, :, :, 25] != 0.0)] = 1.0\n",
    "\n",
    "    lambda_indicator[:, :, :, 20][lambda_indicator[:, :, :, 20] == 0.0] = lambda_noobj\n",
    "    lambda_indicator[:, :, :, 21] = lambda_indicator[:, :, :, 21]*lambda_coord\n",
    "    lambda_indicator[:, :, :, 22] = lambda_indicator[:, :, :, 22]*lambda_coord\n",
    "    lambda_indicator[:, :, :, 23] = lambda_indicator[:, :, :, 23]*lambda_coord\n",
    "    lambda_indicator[:, :, :, 24] = lambda_indicator[:, :, :, 24]*lambda_coord\n",
    "\n",
    "    lambda_indicator[:, :, :, 25][lambda_indicator[:, :, :, 25] == 0.0] = lambda_noobj\n",
    "    lambda_indicator[:, :, :, 26] = lambda_indicator[:, :, :, 26]*lambda_coord\n",
    "    lambda_indicator[:, :, :, 27] = lambda_indicator[:, :, :, 27]*lambda_coord\n",
    "    lambda_indicator[:, :, :, 28] = lambda_indicator[:, :, :, 28]*lambda_coord\n",
    "    lambda_indicator[:, :, :, 29] = lambda_indicator[:, :, :, 29]*lambda_coord\n",
    "    \n",
    "    return lambda_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_hat, indicator):\n",
    "    y[:,:,:,23] = torch.sqrt(y[:,:,:,23])\n",
    "    y[:,:,:,24] = torch.sqrt(y[:,:,:,24])\n",
    "    y[:,:,:,28] = torch.sqrt(y[:,:,:,28])\n",
    "    y[:,:,:,29] = torch.sqrt(y[:,:,:,29])\n",
    "    \n",
    "    y_hat[:,:,:,23] = torch.sqrt(y_hat[:,:,:,23])\n",
    "    y_hat[:,:,:,24] = torch.sqrt(y_hat[:,:,:,24])\n",
    "    y_hat[:,:,:,28] = torch.sqrt(y_hat[:,:,:,28])\n",
    "    y_hat[:,:,:,29] = torch.sqrt(y_hat[:,:,:,29])\n",
    "    \n",
    "    return torch.sum(indicator * ((y - y_hat)*(y - y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_hidden=4096):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Removing fc layer\n",
    "        pretrained_model = list(models.vgg16(pretrained='imagenet').children())[:-1]\n",
    "        self.pretrained_model = nn.Sequential(*pretrained_model)\n",
    "        \n",
    "        self.fc = nn.Linear(512*7*7, n_hidden)\n",
    "        self.fc_out = nn.Linear(n_hidden, S*S*(5*B + C))\n",
    "        \n",
    "        self.drop = nn.Dropout(0.5)        \n",
    "                                      \n",
    "    def forward(self, x):\n",
    "        x = self.pretrained_model(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "        x = x.view(x.size(0), S, S, (5*B + C))\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "    \n",
    "    def change_freezing(self, mode=False):\n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model = Model()\n",
    "model = model.to(device)\n",
    "model.change_freezing(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lambda_indicator = create_lambda_indicator(y_train)\n",
    "val_lambda_indicator = create_lambda_indicator(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_epochs = 40\n",
    "lr = 2e-5\n",
    "wd = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PascalData(train_list, y_train, train_lambda_indicator)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = PascalData(val_list, y_val, val_lambda_indicator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_evaluation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for _, (x_i, y_i, indicator_i) in enumerate(train_loader):\n",
    "        model.zero_grad()\n",
    "        y_hat = model(x_i)\n",
    "        batch_loss = loss(y_i, y_hat, indicator_i)\n",
    "\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += batch_loss / batch_size\n",
    "    train_loss = (train_loss/len(train_loader)).detach().cpu().numpy()\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"----- epoch {epoch} -----\")\n",
    "        print(\"Train loss: {:.4f}\".format(train_loss))\n",
    "\n",
    "        if validation_evaluation:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "\n",
    "            for _, (x_i, y_i, indicator_i) in enumerate(val_loader):\n",
    "                y_hat = model(x_i)\n",
    "                batch_loss = loss(y_i, y_hat, indicator_i)\n",
    "                val_loss += batch_loss / batch_size\n",
    "            val_loss = (val_loss/len(val_loader)).detach().cpu().numpy()\n",
    "\n",
    "            print(\"Val loss: {:.4f}\".format(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 5\n",
    "dataset = train_dataset\n",
    "(x_i, _, _) = dataset[index]\n",
    "\n",
    "model.eval()\n",
    "y_hat = model(x_i.reshape(1, 3, target_size, target_size))\n",
    "y_hat = y_hat.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = target_size / S\n",
    "pc_threshold = 0.25\n",
    "\n",
    "output_bb = []\n",
    "class_id_list = []\n",
    "\n",
    "for i in range(S):\n",
    "    for j in range(S):\n",
    "        found_object = False\n",
    "        \n",
    "        for b in range(B):\n",
    "            pc = y_hat[:, i, j, 20 + 5*b]\n",
    "\n",
    "            if pc < pc_threshold:\n",
    "                continue\n",
    "            else:\n",
    "                found_object = True\n",
    "            \n",
    "            cx = (i + y_hat[:, i, j, 21 + 5*b])*grid_size\n",
    "            cy = (j + y_hat[:, i, j, 22 + 5*b])*grid_size \n",
    "            \n",
    "            box_width = y_hat[:, i, j, 23 + 5*b]*target_size\n",
    "            box_height = y_hat[:, i, j, 24 + 5*b]*target_size\n",
    "            \n",
    "            output_bb.append([pc[0], cx[0], cy[0], box_width[0], box_height[0]])\n",
    "        \n",
    "        if found_object:\n",
    "            class_id_list.append(np.argmax(y_hat[:, i, j, 0:20][0,:]))\n",
    "\n",
    "class_name_list = [id_cat[c] for c in class_id_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(output_bb)):\n",
    "    output_bb[i].insert(1, (class_name_list[i]))\n",
    "    output_bb[i].insert(1, class_id_list[i])\n",
    "    \n",
    "output_bb = sorted(output_bb, key = lambda x: x[0], reverse=True)\n",
    "output_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bb = evaluation.non_max_suppression(output_bb)\n",
    "filtered_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = dataset.file_list[index]\n",
    "img_str = img_path + img_file\n",
    "\n",
    "img = display.read_img(img_str, target_size)\n",
    "img = display.draw_boxes(img, [bb[3:] for bb in filtered_bb])\n",
    "img = display.draw_text(img, [bb[2] for bb in filtered_bb], [bb[3:] for bb in filtered_bb])\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
