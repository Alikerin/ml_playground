{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import PascalVoc, COLOR_2_INDEX, CLASS_NAMES\n",
    "from src.utils import mean_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/VOCdevkit/VOC2012/\"\n",
    "seed = 42\n",
    "\n",
    "img_size = 224\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "train_dataset = PascalVoc(path, img_size, device=device)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class trans_conv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(in_channels, out_channels,\n",
    "                                       kernel_size=3, stride=2,\n",
    "                                       padding=1, output_padding=1)\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class u_net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Indicies used for U-net\n",
    "        self.un_block_indicies = [(0, 4), (5, 9), (10, 16), \n",
    "                                   (17, 23), (24, 30)]\n",
    "        self.init_vgg()\n",
    "\n",
    "        self.conv_7x7_1 = conv2d(512, 512)\n",
    "        self.conv_7x7_2 = conv2d(512, 512)\n",
    "        \n",
    "        self.trans_conv_1 = trans_conv2d(512, 256)\n",
    "        self.trans_conv_2 = trans_conv2d(512, 256)\n",
    "        self.trans_conv_3 = trans_conv2d(512, 128)\n",
    "        self.trans_conv_4 = trans_conv2d(256, 64)\n",
    "        self.trans_conv_5 = trans_conv2d(128, 32)\n",
    "        \n",
    "        self.conv_1 = conv2d(256 + 512, 512)\n",
    "        self.conv_2 = conv2d(256 + 512, 512)\n",
    "        self.conv_3 = conv2d(128 + 256, 256)\n",
    "        self.conv_4 = conv2d(64 + 128, 128)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(32 + 64, num_classes, stride=1, kernel_size=1)\n",
    "        \n",
    "    def init_vgg(self):\n",
    "        model = vgg16(pretrained=True)\n",
    "        del model.classifier\n",
    "        model = model.to(device)\n",
    "        \n",
    "        self.blocks = []\n",
    "        \n",
    "        for block_idx in self.un_block_indicies:\n",
    "            self.blocks.append(model.features[block_idx[0]:block_idx[1]])\n",
    "        \n",
    "        # Freezing the VGG weights\n",
    "        for block in self.blocks:\n",
    "            for param in block.parameters():\n",
    "                param.requires_grad = False        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_1 = self.blocks[0](x)\n",
    "        encoder = F.max_pool2d(x_1, kernel_size=2, stride=2)\n",
    "        x_2 = self.blocks[1](encoder)\n",
    "        encoder = F.max_pool2d(x_2, kernel_size=2, stride=2)\n",
    "        x_3 = self.blocks[2](encoder)\n",
    "        encoder = F.max_pool2d(x_3, kernel_size=2, stride=2)\n",
    "        x_4 = self.blocks[3](encoder)\n",
    "        encoder = F.max_pool2d(x_4, kernel_size=2, stride=2)\n",
    "        x_5 = self.blocks[4](encoder)\n",
    "        encoder = F.max_pool2d(x_5, kernel_size=2, stride=2)\n",
    "        \n",
    "        encoder = self.conv_7x7_1(encoder)\n",
    "        encoder = self.conv_7x7_2(encoder)\n",
    "        \n",
    "        decoder = self.trans_conv_1(encoder)\n",
    "        decoder = torch.cat((decoder, x_5), dim=1)\n",
    "        decoder = self.conv_1(decoder)\n",
    "        \n",
    "        decoder = self.trans_conv_2(decoder)\n",
    "        decoder = torch.cat((decoder, x_4), dim=1)\n",
    "        decoder = self.conv_2(decoder)\n",
    "        \n",
    "        decoder = self.trans_conv_3(decoder)\n",
    "        decoder = torch.cat((decoder, x_3), dim=1)\n",
    "        decoder = self.conv_3(decoder)\n",
    "        \n",
    "        decoder = self.trans_conv_4(decoder)\n",
    "        decoder = torch.cat((decoder, x_2), dim=1)\n",
    "        decoder = self.conv_4(decoder)\n",
    "        \n",
    "        decoder = self.trans_conv_5(decoder)\n",
    "        decoder = torch.cat((decoder, x_1), dim=1)\n",
    "        decoder = self.out_conv(decoder)\n",
    "        \n",
    "        score = F.log_softmax(decoder, dim=1)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = u_net(len(CLASS_NAMES)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "\n",
    "lr = 2e-3\n",
    "wd = 1e-5\n",
    "gamma = 0.5\n",
    "step_size = 10\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "loss_fct = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model.train()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss = 0.0\n",
    "    iou = 0.0\n",
    "    \n",
    "    scheduler.step()\n",
    "    for _, (x, y) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_hat = model(x)\n",
    "        batch_loss = loss_fct(y_hat, y)\n",
    "        \n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = batch_loss.detach().cpu().numpy()\n",
    "        loss += batch_loss\n",
    "        \n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        batch_iou = mean_iou(preds, y).detach().cpu().numpy()        \n",
    "        iou += batch_iou\n",
    "    \n",
    "    loss = loss / len(train_dataloader)\n",
    "    iou = iou / len(train_dataloader)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"-------Epoch {epoch}-------\")\n",
    "        print(f\"Loss : {loss}\")\n",
    "        print(f\"Mean IOU: {iou}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "model.eval()\n",
    "\n",
    "idx = 3\n",
    "x, _ = train_dataset[idx]\n",
    "y, _ = train_dataset.load_imgs(idx)\n",
    "gt = train_dataset.segmentation_imgs[idx]\n",
    "gt = Image.open(gt).convert(\"RGB\")\n",
    "\n",
    "x = x.unsqueeze(0)\n",
    "y_hat = model(x)\n",
    "y_hat = torch.argmax(y_hat, dim=1)[0, :, :]\n",
    "y_hat = y_hat.detach().cpu().numpy()\n",
    "\n",
    "gt = gt.resize((img_size, img_size))\n",
    "gt = np.array(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_mask = np.zeros((img_size, img_size, 3))\n",
    "\n",
    "for i in range(img_size):\n",
    "    for j in range(img_size):\n",
    "        y_hat_mask[i, j, :] = COLOR_2_INDEX[y_hat[i, j]]\n",
    "        \n",
    "y_hat_mask = y_hat_mask.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 15, 15\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(y)\n",
    "plt.title(\"Original image\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(gt)\n",
    "plt.title(\"Ground truth segmentation\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(y_hat_mask)\n",
    "plt.title(\"Predicted segmentation\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
